{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path containing the images\n",
    "data_directory = \"/Users/lukasiwan/NeueFische/Repositories/Hydroponics/data/train_data\"\n",
    "batch_size = 7\n",
    "image_size = (50,50)\n",
    "epoch_size = 250\n",
    "fold_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KFold object\n",
    "kfold = KFold(n_splits=fold_size, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the fold results\n",
    "fold_train_loss = []\n",
    "fold_train_accuracy = []\n",
    "fold_val_loss = []\n",
    "fold_val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "fold = 1\n",
    "for train_index, val_index in kfold.split(os.listdir(data_directory)):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Load and preprocess the data for training set\n",
    "    train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_directory,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\"\n",
    "    )\n",
    "\n",
    "    # Load and preprocess the data for validation set\n",
    "    val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_directory,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\"\n",
    "    )\n",
    "\n",
    "    # Define data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      preprocessing.Rescaling(1./255),\n",
    "      preprocessing.Resizing(image_size[0], image_size[1]),\n",
    "      preprocessing.RandomFlip(\"horizontal\"),\n",
    "      preprocessing.RandomRotation(0.3), \n",
    "      #preprocessing.RandomZoom(0.1)\n",
    "    ])\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        data_augmentation,\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_data, validation_data=val_data, epochs=epoch_size)\n",
    "\n",
    "    # Store the fold results\n",
    "    fold_train_loss.append(history.history['loss'])\n",
    "    fold_train_accuracy.append(history.history['accuracy'])\n",
    "    fold_val_loss.append(history.history['val_loss'])\n",
    "    fold_val_accuracy.append(history.history['val_accuracy'])\n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(val_data)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Get training history\n",
    "train_loss = history.history['loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create line plots\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.title(f'Training and Validation Accuracy\\n(Epochs: {epoch_size}, Batch Size: {batch_size}, Image Size: {image_size[0]}x{image_size[1]})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title(f'Training and Validation Loss\\n(Epochs: {epoch_size}, Batch Size: {batch_size}, Image Size: {image_size[0]}x{image_size[1]})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
