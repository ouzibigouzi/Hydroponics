{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the scaling of the baseline model, e.g. normalization and standardization with the ImageDataGenerator.\n",
    "\n",
    "The first part will explain the different steps of scaling, afterwards both scaling methods will be used on the baseline model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview scaling with the ImageDataGenerator__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are done with the data split into training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator and input the chosen scaling choices (also augmentation is possible)\n",
    "# for Normalization: (rescale = 1.0/255.0)\n",
    "# for Standardization (this includes Centering): (featurewise_center = True, featurewise_std_normalization = True)\n",
    "datagen= ImageDataGenerator(rescale = 1.0/255.0) #this would be normalization\n",
    "\n",
    "# if needed (depends on scaling method), calculate for the whole training data set the statistics using the .fit() function. Later on this can be applied to test and validation data set.\n",
    "datagen.fit(X_train)\n",
    "\n",
    "print('Train min=%.3f, max=%.3f' % (X_train.min(), X_train.max()))\n",
    "\n",
    "# A neural network model can be fitted with the data generator by using .flow() . It retrieves an iterator which returns batches of data and passes it to the fit_generator() function.\n",
    "\n",
    "# creating the iterator, choose the wanted batch size\n",
    "train_iterator = datagen_normalization.flow(X_train, y_train, batch_size = 64)\n",
    "\n",
    "\n",
    "# Optional: confirm that the iterators and the scaling work\n",
    "batchX, batchy = train_iterator.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(),\n",
    "                                              batchX.max()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() is applied to the model and the train_iterator is chosen. It will take the scaled data and feed it into the model.\n",
    "A hyperparameter here is the number of epochs and the steps per epoch, which can be chosen accordingly.\n",
    "\n",
    "For larger datasets the function fit_generator() can be used. It will divide the data into batches and scale in-time during training while feeding the batches to the model. For the fit() function the whole data would be stored in RAM, which is not always possible for larger or more complex data sets.\n",
    "\n",
    "For later plotting purposes the output is additionally stored in history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to evaluated the model with the validation data.\n",
    "First the validation data part from the read in data (see subset = \"validation\") is chosen and prepared in the same way as the train data. \n",
    "A validation iterator is created and finally the model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model is validated\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "X_val = np.concatenate([x for x, _ in val_data], axis=0)\n",
    "y_val = np.concatenate([y for _, y in val_data], axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_val = to_categorical(y_val, num_classes=4)\n",
    "\n",
    "# an iterator is created from the validation data.\n",
    "val_iterator = datagen.flow(X_val, y_val, batch_size = 64) \n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In the following are some preparations for the baseline model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# converts a class vector (integers) to binary class matrix \n",
    "from keras.utils import to_categorical\n",
    "# The ImageDataGenerator itself\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading in the data with image_dataset_from_directory__\n",
    "\n",
    "This splits the data into train and validation data (later used) and also resizes the images to the given image_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path containing the images\n",
    "data_directory = \"/Users/linn/Desktop/original_dataset\"\n",
    "batch_size = 32\n",
    "image_size = (200,200)\n",
    "data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    batch_size=batch_size,\n",
    "    image_size = image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class names\n",
    "class_names = data.class_names\n",
    "print(\"Class names:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "X_train = np.concatenate([x for x, _ in data], axis=0)\n",
    "y_train = np.concatenate([y for _, y in data], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoded format\n",
    "y_train = to_categorical(y_train, num_classes=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the baseline model architecture\n",
    "model_normalization = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_normalization.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_normalization.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a normalization is tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator and input the chosen scaling choices \n",
    "datagen_normalization = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "# the data set is scaled\n",
    "datagen_normalization.fit(X_train)\n",
    "\n",
    "# A neural network model can be fitted with the data generator by using .flow() . It retrieves an iterator which returns batches of data and passes it to the fit_generator() function.\n",
    "\n",
    "# creating the iterator, choose the wanted batch size\n",
    "train_iterator = datagen_normalization.flow(X_train, y_train, batch_size = 64)\n",
    "\n",
    "# confirming scaling\n",
    "batchX, batchy = train_iterator.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(),\n",
    "                                              batchX.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = model_normalization.fit(train_iterator, steps_per_epoch=len(train_iterator), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model is validated\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "X_val = np.concatenate([x for x, _ in val_data], axis=0)\n",
    "y_val = np.concatenate([y for _, y in val_data], axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_val = to_categorical(y_val, num_classes=4)\n",
    "\n",
    "val_iterator = datagen_normalization.flow(X_val, y_val, batch_size = 64) \n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model_normalization.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy unfortunately goes down in validation to 25%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is another type of validation, it achieves the same accuracy (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model_normalization.evaluate_generator(val_iterator, steps=len(val_iterator), verbose=0)\n",
    "print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history_val[val_accuracy])\n",
    "plt.title('model accuracy normalization')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(val_)\n",
    "plt.title('model loss normalization')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the standardization is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the baseline model architecture\n",
    "model_standardization = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_standardization.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_standardization.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator and input the chosen scaling choices (\n",
    "# for Standardization (this includes Centering): (featurewise_center = True, featurewise_std_normalization = True)\n",
    "datagen_standardization = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n",
    "\n",
    "# fit scaling on the train data\n",
    "datagen_standardization.fit(X_train)\n",
    "\n",
    "# creating the iterator, choose the wanted batch size\n",
    "train_iterator_std = datagen_standardization.flow(X_train, y_train, batch_size = 64)\n",
    "\n",
    "\n",
    "# confirm that the iterator works\n",
    "batchX, batchy = train_iterator_std.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(),\n",
    "                                              batchX.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "history_std = model_standardization.fit(train_iterator_std, steps_per_epoch=len(train_iterator), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss and accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_std.history['accuracy'])\n",
    "plt.title('model accuracy standardization')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_std.history['loss'])\n",
    "plt.title('model loss standardization')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted model is validated\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "X_val = np.concatenate([x for x, _ in val_data], axis=0)\n",
    "y_val = np.concatenate([y for _, y in val_data], axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_val = to_categorical(y_val, num_classes=4)\n",
    "\n",
    "val_iterator_std = datagen_standardization.flow(X_val, y_val, batch_size = 64) \n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model_standardization.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy with standardization is 28%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is also evaluated with another approach with the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model_standardization.evaluate_generator(val_iterator_std, steps=len(val_iterator_std), verbose=0)\n",
    "print('Test Accuracy: %.3f' % (acc * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization and Normalization do not improve the model accuracy.\n",
    "Maybe another opmtimization tool is needed as well for the scaling to improve the accuracy.\n",
    "Baseline model: 33%\n",
    "Scaled with normalization: 25%\n",
    "Scaled with standardization: 28%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
