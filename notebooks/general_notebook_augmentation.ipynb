{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/richardwang/Documents/Hydroponics/original_dataset/\"\n",
    "data_dir = Path(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format of all the images in the input directory to the specified format. Can be used on single images or a directory of images.\n",
    "\n",
    "prep.change_format(input_dir, \"png\", keep= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset with a pre-defined pipeline\n",
    "data_original = tf.keras.utils.image_dataset_from_directory(input_dir) # not all image formats supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = data_original.class_names\n",
    "data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of images\n",
    "\n",
    "healthy = list(data_dir.glob('healthy/*'))\n",
    "nitro = list(data_dir.glob('deficiency_nitrogen/*'))\n",
    "phos = list(data_dir.glob('deficiency_phosphorus/*'))\n",
    "pota = list(data_dir.glob('deficiency_potassium/*'))\n",
    "\n",
    "for image in phos[-5:]:\n",
    "    display.display(Image.open(str(image)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell creates a folder for the test data that is randomly selected from the original dataset. Please only \n",
    "# run this cell once, as it will fill the test folder with images.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# directories\n",
    "src_dir = input_dir\n",
    "test_dir = \"/Users/richardwang/Documents/Hydroponics/test_data\"\n",
    "\n",
    "# make sure destination directory exists. If it already exists, pass this step.\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # proportion of images to use for testing\n",
    "    test_ratio = 0.2\n",
    "\n",
    "    # seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # for each class directory in the source directory\n",
    "    for class_dir in os.listdir(src_dir):\n",
    "        if os.path.isdir(os.path.join(src_dir, class_dir)):\n",
    "            # list all image files in this class directory\n",
    "            images = os.listdir(os.path.join(src_dir, class_dir))\n",
    "            \n",
    "            # randomly shuffle the images\n",
    "            np.random.shuffle(images)\n",
    "            \n",
    "            # determine the number of images to move\n",
    "            n_test_images = int(len(images) * test_ratio)\n",
    "            \n",
    "            # create a new directory for this class in the destination directory\n",
    "            os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
    "            \n",
    "            # move the test images to the destination directory\n",
    "            for image in images[:n_test_images]:\n",
    "                shutil.move(os.path.join(src_dir, class_dir, image), os.path.join(test_dir, class_dir, image))\n",
    "else:\n",
    "    print(\"Test data already exists. This step is skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "image_size = (256, 256)\n",
    "s1,s2 = image_size\n",
    "\n",
    "test_dir = \"/Users/richardwang/Documents/Hydroponics/test_data\"\n",
    "\n",
    "datagen= ImageDataGenerator(\n",
    "    rotation_range=135,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range = 0.05,\n",
    "    zoom_range = 0.5,\n",
    "    rescale= 1./255,\n",
    "    # brightness_range =  (0.0,1),\n",
    "    fill_mode=\"constant\", # cval is the value used for points outside the boundaries when fill_mode is \"constant\"\n",
    "                        # besides \"constant\" there are \"nearest\", \"reflect\" and \"wrap\". the fill_mode is used when the image is augmented and the size of the image changes\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(directory=input_dir, batch_size=batch_size, target_size=image_size, color_mode=\"rgb\", class_mode = 'sparse')\n",
    "val_generator = val_datagen.flow_from_directory(directory= input_dir, batch_size=batch_size, target_size=image_size, color_mode=\"rgb\", class_mode = 'sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=( s1, s2, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              # loss='sparse_categorical_crossentropy',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.n // batch_size\n",
    "num_epochs = 30\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs)\n",
    "\n",
    "    \n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator, \n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "# # Train the model\n",
    "# history = model.fit(data, validation_data=val_data, epochs=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data \n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get training history\n",
    "train_loss = history.history['loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create line plots\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# load ResNet50 model without classification layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add new classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # add a global spatial average pooling layer\n",
    "x = Dense(1024, activation='relu')(x)  # let's add a fully-connected layer\n",
    "predictions = Dense(4, activation='softmax')(x)  # we have 4 classes\n",
    "\n",
    "# define the new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss= tf.keras.losses.SparseCategoricalCrossentropy() , metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
