{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/richardwang/.pyenv/versions/3.11.3/lib/python3.11/site-packages (9.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " #* these libraries will be put in the requeriments.txt file, stay her for now\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install opencv-python\n",
    "!pip install pillow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/richardwang/Documents/Hydroponics/original_dataset/\"\n",
    "data_dir = Path(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format of all the images in the input directory to the specified format. Can be used on single images or a directory of images.\n",
    "\n",
    "prep.change_format(input_dir, \"png\", keep= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset with a pre-defined pipeline\n",
    "data_original = tf.keras.utils.image_dataset_from_directory(input_dir) # not all image formats supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deficiency_nitrogen',\n",
       " 'deficiency_phosphorus',\n",
       " 'deficiency_potassium',\n",
       " 'healthy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classes = data_original.class_names\n",
    "data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of images\n",
    "\n",
    "\n",
    "healthy = list(data_dir.glob('healthy/*'))\n",
    "nitro = list(data_dir.glob('deficiency_nitrogen/*'))\n",
    "phos = list(data_dir.glob('deficiency_phosphorus/*'))\n",
    "pota = list(data_dir.glob('deficiency_potassium/*'))\n",
    "\n",
    "for image in healthy[-5:]:\n",
    "    display.display(Image.open(str(image)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    " #* in this section we can see how to use the ImageDataGenerator to augment the images. Use it as playglround\n",
    "\n",
    "datagen= ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    rescale= 1./255,\n",
    "    fill_mode = 'constant', cval = 255\n",
    ")\n",
    " #* we define the (single) imagefile we want to augment\n",
    "imagefile = ('/Users/richardwang/Documents/Hydroponics/original_dataset/deficiency_nitrogen/hydroponics_lettuce_P   _01.png')\n",
    "#we read the imagefile with opencv\n",
    "x = cv2.imread(imagefile)\n",
    "\n",
    "#we reshape the image to (1, 256, 256, 3) to fit the input of the model\n",
    "x = x.reshape((1,) + x.shape)\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=16,\n",
    "                          save_to_dir='/Users/richardwang/Documents/Hydroponics/original_dataset/', save_prefix='aug', save_format='png'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 files belonging to 4 classes.\n",
      "Using 225 files for training.\n",
      "Class names: ['deficiency_nitrogen', 'deficiency_phosphorus', 'deficiency_potassium', 'healthy']\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_45 (Conv2D)          (None, 448, 448, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_45 (MaxPooli  (None, 224, 224, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 222, 222, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_46 (MaxPooli  (None, 111, 111, 64)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 109, 109, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_47 (MaxPooli  (None, 54, 54, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 373248)            0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               47775872  \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47869636 (182.61 MB)\n",
      "Trainable params: 47869636 (182.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 22.2382 - accuracy: 0.2667 \n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4821 - accuracy: 0.2222 \n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4324 - accuracy: 0.2667 \n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4007 - accuracy: 0.2578 \n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4517 - accuracy: 0.3022 \n",
      "Epoch 6/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.3962 - accuracy: 0.3022  \n",
      "Epoch 7/40\n",
      "7/7 [==============================] - 7s 953ms/step - loss: 1.4734 - accuracy: 0.2978\n",
      "Epoch 8/40\n",
      "7/7 [==============================] - 7s 957ms/step - loss: 1.5796 - accuracy: 0.2311\n",
      "Epoch 9/40\n",
      "7/7 [==============================] - 7s 895ms/step - loss: 1.4565 - accuracy: 0.2756\n",
      "Epoch 10/40\n",
      "7/7 [==============================] - 7s 891ms/step - loss: 1.4425 - accuracy: 0.2800\n",
      "Epoch 11/40\n",
      "7/7 [==============================] - 7s 911ms/step - loss: 1.4593 - accuracy: 0.2844\n",
      "Epoch 12/40\n",
      "7/7 [==============================] - 7s 1s/step - loss: 1.4605 - accuracy: 0.1956 \n",
      "Epoch 13/40\n",
      "7/7 [==============================] - 7s 892ms/step - loss: 1.3973 - accuracy: 0.2978\n",
      "Epoch 14/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.3730 - accuracy: 0.3067 \n",
      "Epoch 15/40\n",
      "7/7 [==============================] - 7s 1s/step - loss: 1.3554 - accuracy: 0.3289  \n",
      "Epoch 16/40\n",
      "7/7 [==============================] - 7s 936ms/step - loss: 1.3772 - accuracy: 0.3244\n",
      "Epoch 17/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4484 - accuracy: 0.3511  \n",
      "Epoch 18/40\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.3802 - accuracy: 0.3067 \n",
      "Epoch 19/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.4138 - accuracy: 0.2844 \n",
      "Epoch 20/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.3821 - accuracy: 0.2889 \n",
      "Epoch 21/40\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.4732 - accuracy: 0.2622 \n",
      "Epoch 22/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.4268 - accuracy: 0.2711  \n",
      "Epoch 23/40\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.4792 - accuracy: 0.2889 \n",
      "Epoch 24/40\n",
      "7/7 [==============================] - 11s 1s/step - loss: 1.3573 - accuracy: 0.3333 \n",
      "Epoch 25/40\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.3868 - accuracy: 0.2978 \n",
      "Epoch 26/40\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.3576 - accuracy: 0.3333 \n",
      "Epoch 27/40\n",
      "7/7 [==============================] - 9s 2s/step - loss: 1.3852 - accuracy: 0.3067  \n",
      "Epoch 28/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.5083 - accuracy: 0.2889  \n",
      "Epoch 29/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.6466 - accuracy: 0.2578  \n",
      "Epoch 30/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.6695 - accuracy: 0.2756  \n",
      "Epoch 31/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.7005 - accuracy: 0.2667  \n",
      "Epoch 32/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.6819 - accuracy: 0.3378  \n",
      "Epoch 33/40\n",
      "7/7 [==============================] - 10s 2s/step - loss: 1.6793 - accuracy: 0.2667 \n",
      "Epoch 34/40\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.4395 - accuracy: 0.2800 \n",
      "Epoch 35/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.5251 - accuracy: 0.3022  \n",
      "Epoch 36/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.3854 - accuracy: 0.2889 \n",
      "Epoch 37/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.3762 - accuracy: 0.2800  \n",
      "Epoch 38/40\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.3835 - accuracy: 0.2711 \n",
      "Epoch 39/40\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.4640 - accuracy: 0.3022 \n",
      "Epoch 40/40\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.5320 - accuracy: 0.3333 \n",
      "Found 281 files belonging to 4 classes.\n",
      "Using 56 files for validation.\n",
      "2/2 [==============================] - 1s 262ms/step - loss: 73.0842 - accuracy: 0.2500\n",
      "Validation Loss: 73.08418273925781\n",
      "Validation Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define batch and image size here, because it is used in multiple places\n",
    "batch_size = 32\n",
    "image_size = (450, 450)\n",
    "\n",
    "data = tf.keras.utils.image_dataset_from_directory(\n",
    "    input_dir,\n",
    "    batch_size= batch_size,\n",
    "    image_size = image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2, # 20% of the data will automatically be used for validation\n",
    "    subset=\"training\")\n",
    "\n",
    "\n",
    "\n",
    "# Print the class names\n",
    "class_names = data.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "X_train = np.concatenate([x for x, _ in data], axis=0)\n",
    "y_train = np.concatenate([y for _, y in data], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train = to_categorical(y_train, num_classes=4)\n",
    "\n",
    "\n",
    "augmented= ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    rescale= 1./255,\n",
    "    fill_mode = 'constant', cval = 20\n",
    ")\n",
    "\n",
    "# Fit the augmentation method to the data\n",
    "augmented.fit(X_train)\n",
    "\n",
    "s1, s2 = image_size\n",
    "# Define the baseline model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=( s1, s2, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(augmented.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train) / batch_size, epochs=40)\n",
    "\n",
    "\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    input_dir,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "X_val = np.concatenate([x for x, _ in val_data], axis=0)\n",
    "y_val = np.concatenate([y for _, y in val_data], axis=0)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_val = to_categorical(y_val, num_classes=4)\n",
    "\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
